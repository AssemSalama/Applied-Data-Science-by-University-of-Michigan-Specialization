{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Semantic Text Similarity.\n",
    "![1](1.png)\n",
    "![2](2.png)\n",
    "- `Paraphrasing` is a task where you rephrase or rewrite some sentence you get into another sentence that has the same meaning.\n",
    "- `Textual entailment`, on the other hand, is a little bit more complex. It says that the smaller sentence or one of the two sentences derives its meaning or entails its meaning from another piece of text. So you have a text document or a text passage and a sentence. And based on the information in the text passage, you need to say whether the sentence is correct or it derives its meaning from there or not. This is a typical task of semantic similarity.\n",
    "\n",
    "**One of the resources useful for semantic similarity is WordNet.**\n",
    "![3](3.png)\n",
    "![4](4.png)\n",
    "![5](5.png)\n",
    "![6](6.png)\n",
    "# **pathsim = 1 / (1+distance)**\n",
    "![7](7.png)\n",
    "![8](8.png)\n",
    "![9](9.png)\n",
    "![10](10.png)\n",
    "**It says I want deer in the sense of given by the noun meaning of it and the first meaning of that. The same way with elk, you find the synset that corresponds to elk.n.01 and so on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deer vs. elk] path sim = 0.5\n",
      "[deer vs. horse] path sim: 0.14285714285714285\n",
      "\n",
      "[deer vs. elk] Lin sim = 0.8623778273893673\n",
      "[deer vs. horse] Lin sim = 0.7726998936065773\n"
     ]
    }
   ],
   "source": [
    "# Find path similarity. \n",
    "print('[deer vs. elk] path sim =',deer.path_similarity(elk))\n",
    "print('[deer vs. horse] path sim:',deer.path_similarity(horse))\n",
    "\n",
    "# Use an information criteria to find Lin similarity.\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "print('\\n[deer vs. elk] Lin sim =',deer.lin_similarity(elk,brown_ic))\n",
    "print('[deer vs. horse] Lin sim =',deer.lin_similarity(horse,brown_ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I don't know why i did't get the results the instructor got. On top of that, this is the instructor's comment: \n",
    "*And you'll notice especially here, that this is not using the distance between two concepts explicitly. So deer and horse, that were very far away in the WordNet hierarchy still get the higher significance and higher similarity between them. And that is because, in typical contexts and the information that is contained by these words deer and horse, you have deer and horse are enough closer in similarity because they are both basically mammals. But Elk is a very specific instance of deer and not necessarily, in the particular Lin similarity doesn't come out as close.*\n",
    "![12](12.png)\n",
    "![13](13.png)\n",
    "![14](14.png)\n",
    "- There is a way in which you can normalize such that this very frequent word does not kind of, super ride all the other similarity measures you find. And one way to do it would be using Pointwise Mutual Information.\n",
    "![15](15.png)\n",
    "\n",
    "\n",
    "1. *You can define bigrams as NLTK collocations bigrams, bigram association measures, and then you can learn that based on a corpus. In this case, given as text here, so `text corpus` and then, using the PMI measure you can say, I'm going to get the top 10 pairs using the PMI measure from bigram_measures.*\n",
    "\n",
    "\n",
    "2. *You can use a Use Finder for other useful tasks such as frequency filtering. So suppose you want all bigram measures that are, there you have supposed 10 or more occurrences of words only then can you keep them, then you could do something like `finder.apply_ freq_filter (10)`. That would then restrict any pair that `does not occur at least 10 times in your corpus`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ashteroth', 'Karnaim'),\n",
       " ('Hitti', 'Which'),\n",
       " ('Philistim', ',)'),\n",
       " ('Thirty', 'milch'),\n",
       " ('Whoso', 'sheddeth'),\n",
       " ('bake', 'unleavened'),\n",
       " ('burning', 'lamp'),\n",
       " ('fig', 'leaves'),\n",
       " ('ford', 'Jabbok'),\n",
       " ('horse', 'heels')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text3)\n",
    "finder.nbest(bigram_measures.pmi , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Now', 'therefore'),\n",
       " ('an', 'altar'),\n",
       " ('every', 'living'),\n",
       " ('lifted', 'up'),\n",
       " ('these', 'things'),\n",
       " ('rose', 'up'),\n",
       " ('These', 'are'),\n",
       " ('did', 'eat'),\n",
       " ('thirty', 'years'),\n",
       " ('every', 'beast')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![16](16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Topic Modeling.\n",
    "![17](17.png)\n",
    "![18](18.png)\n",
    "![19](19.png)\n",
    "![20](20.png)\n",
    "![21](21.png)\n",
    "![22](22.png)\n",
    "![23](23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Generative Models and LDA.\n",
    "![24](24.png)\n",
    "![25](25.png)\n",
    "![26](26.png)\n",
    "- *Suppose you decide that for a particular document, 40% of the words come from topic A, then you use that topic A's multinomial distribution to output the 40% of the words.*\n",
    "![27](27.png)\n",
    "![28](28.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![29](29.png)\n",
    "\n",
    "- Stop words are common words that occur frequently in a particular domain and is not meaningful in that domain. So for example, in general English, the word `the` and `is` and so on might be words that you want to remove.\n",
    "\n",
    "\n",
    "- While if you are in the area of medical documents, let's say, so clinical notes, you would always see the word `patient` and always see the word `doctor` and so on. And they may not be as important as the other words, like what is the medication and what is the disease. Then you may want to say patient and doctor are stop words for that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "# 1:\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "lower_tokens = [token.lower() for token in moby_tokens]\n",
    "\n",
    "# 2: N/G\n",
    "\n",
    "# 3:\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "lemmatized_tokens = [WNlemma.lemmatize(token) for token in lower_tokens]\n",
    "\n",
    "# 4:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(lemmatized_tokens)\n",
    "lemm_tokens_vectorized = vect.transform(lemmatized_tokens)\n",
    "doc = lemm_tokens_vectorized\n",
    "\n",
    "# 5:\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "\n",
    "# dictionary = corpora.Dictionary(doc)\n",
    "# TypeError: decoding to str: need a bytes-like object, csr_matrix found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![30](30.png)\n",
    "\n",
    "1. So once you have built the mapping between the terms and documents, then suppose you have a set of pre-processed text documents in this variable doc_set. Then you could use gensim to learn LDA this way. You could import gensim and specifically you import the corpora and the models. First you create a dictionary, dictionary is mapping between IDs and words.\n",
    "\n",
    "\n",
    "2. Then you create corpus, and corpus you create going through this, all the documents in the doc_set, and creating a document to bag of words model. This is the step that creates the document term matrix. Once you have that, then you input that in the LdaModel call, so that you use a gensim.models LdaModel, where you also specify the number of topics you want to learn. So in this case, we said number of topics is going to be four, and you also specify this mapping, the id2word mapping. That's a dictionary that is learned two steps ahead.\n",
    "\n",
    "\n",
    "3. Once you have learned that, then that's it, and you can say how many passes it should go through. And there are other parameters that I would encourage you to read upon. But once you have defined this ldamodel, you can then use the ldamodel to print the topics. So, in this particular case, we learnt four topics. And you can say, give me the top five words of these four topics and then it will bring that one out for you.\n",
    "\n",
    "\n",
    "4. ldamodel model can also be used to find topic distributions of documents. So when you have a new document and you apply the ldamodel on it, so you infer it. You can say, what was the topic distribution, across these four topics, for that new document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![31](31.png)\n",
    "\n",
    "- So for example, if you want to remove all features that are coming from words that are very fairly common in your corpus or you want to focus your features to only those that are coming from specific topics. Then you would want to first train an LDA model. And then, based on just the words that come from specific topics of interest, you might actually generate features that way.\n",
    "\n",
    "- So in general, LDA is a very powerful tool and a text clustering tool that is fairly commonly used as the first step to understand what a corpus is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Quiz.\n",
    "### 1.\n",
    "In the WordNet hierarchy, the word ‘puppy’ is a direct hyponym of ‘dog’ (i.e. ‘puppy’ is a kind of ‘dog’. The least common subsumer for ‘puppy’ and ‘dog’ is:\n",
    "- Answer : Dog\n",
    "\n",
    "### 2.\n",
    "If ‘puppy’ is a direct hyponym of ‘dog’, ‘dog’ is a direct ______ of ‘puppy’\n",
    "- Answer : Hypernym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Information Extraction.\n",
    "![32](32.png)\n",
    "- And that is where information Extraction comes in.\n",
    "![33](33.png)\n",
    "![34](34.png)\n",
    "![35](35.png)\n",
    "![36](36.png)\n",
    "So for example, if you have the word Chicago, it could be a place, it could be an album, it could be a font. So depending on what kind of variation it is, you need to know what is the label it should be assigned to this word. Once you identify that Chicago is your element phrase.\n",
    "\n",
    "![37](37.png)\n",
    "- And certainly, you come to a conclusion where you can see that these named entities don't have to be separate from each other. So for example, bilateral hand numbness is a completely valid named entity in itself. But hand is also a named entity that is of interest. So then the question becomes, what is the granularity with which you need to annotate this.\n",
    "\n",
    "\n",
    "- And if you're doing hand as a separate annotation, why not C5-6 disc or cord because you're talking about spinal cord in this case, you're talking about an anatomical part.\n",
    "\n",
    "\n",
    "- So these are the decisions that need to be made when you're defining this named entity task. So in this particular case, if you define the task to be one, where you say we are only interested in the condition, the diagnosis, the age, the gender, the procedures that were done, and the specialty. That's it. In this then, you're not going to focus on a body part, hand and feet, and even if they are there, that's fine.\n",
    "\n",
    "![38](38.png)\n",
    "\n",
    " - For other fields, it's fairly common to use a machine learning approach. In fact, even for dates and phone numbers you might want to use a machine learning approach, where you use these regular expressions as features. But you have other features that help them define this particular extraction to be valid.\n",
    "\n",
    "\n",
    " - So for example, phone numbers and fax numbers are very similar.The only thing that differentiates them is what is the number preceded by. So is it phone dot, or is it fax dot. So some partition information kind of helps doing that, and you could either include it as a regular expression or let those be features in a machine learning model that you then train over large data sets.\n",
    " \n",
    "![39](39.png)\n",
    "### What outside means ?\n",
    "- If you have a sentence like, John met Brenda. You want to say that John and Brenda are named entities so you have those two persons. And then Matt is a word that is outside of any named entity, so you are going to use these two classes. So you're going to say John is a person, met is outside and Brenda is a person again.\n",
    "\n",
    "![40](40.png)\n",
    "\n",
    "- Erbitux is colored yellow here to represent that it is a treatment of some kind. And then you have lung cancer that is represented, that's in green to represent a different name identity. In this particular case, a disease or a diagnosis.\n",
    "\n",
    "\n",
    "- And then you have a relationship between them. So Erbitux and lung cancer are linked by a treatment relation. Going from Erbitux to lung cancer, that says Erbitux is a treatment for lung cancer. You could easily have a link going the other way. Lung cancer and to Erbitux, lung cancer is treated by Erbitux. So this is the relation, a very simple relation, a binder relation between a disease and a treatment, or a drug.\n",
    "\n",
    "![41](41.png)\n",
    "**- If they are referring to the same entity.**\n",
    "\n",
    "![42](42.png)\n",
    "![43](43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. So for example, if the entity that you need to recognize is a date, you are using typically expressions that we've talked about in `week one`.\n",
    "\n",
    "\n",
    "2. If you are talking about extracting people name. So person name or organization name, you are not only using emotional learning model to identify what is an identity and what label you should get it, but also the features that you're going to use are coming from what we talked about in `week two`. \n",
    "\n",
    "\n",
    "3. So, for example, we want to know that, yes, if it is capitalized or not, but what is the part of speech of a particular word? Is it a noun or a verb? What is the semantic role that a particular word is playing in a given context in a sentence? And these could be features that you would then put in in a named entity recognition model.\n",
    "\n",
    "\n",
    "4. NLTK has an in-built NER model that does trained, or new datasets and so on, for the standard task for the person, organization, location task. But I have shown in this video the named entity recognition problem goes beyond just news. If your even extending it to finance or then if you go all the way to medicine, it's a completely open problem. Where the topics that we have talked about in this course kind of puts it all together and brings it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Quiz:\n",
    "1. Which of the following is not included in WordNet:\n",
    " - `Pronunciation`\n",
    "\n",
    "2. If the shortest distance between words A and B in the WordNet hierarchy is 6, the path-based similarity measure PathSim(A,B) would be:\n",
    " - `1/(6+1) = 1/7 = 0.143`\n",
    "\n",
    "3. When computing the similarity between words C and D, the distributional similarity based metric gives a higher score than a path-based similarity measure. What can be inferred from this observation? \n",
    "  - `Nothing can be inferred. The similarity values from different measures cannot be directly compared`.\n",
    "\n",
    "\n",
    "4. Which of the following approaches can be used to recognize monetary values (like USD 100, JPY 1000, etc.)\n",
    " - `Building regular expressions , Looking up entries in a list and Checking if something is capitalized or title cased`.\n",
    "\n",
    "5. Which of the following is not given as input for a topic modeling setup?\n",
    " - `The topics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
