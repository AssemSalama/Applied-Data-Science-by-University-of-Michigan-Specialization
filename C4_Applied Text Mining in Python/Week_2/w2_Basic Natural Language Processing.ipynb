{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Basic Natural Language Processing.\n",
    "![1](1.png)\n",
    "![2](2.png)\n",
    "![3](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Basic NLP tasks with NLTK.\n",
    "![4](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Counting vocabulary of words\n",
    "`Corpus = Collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the text 1\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "# lets discover the catalog of sentences.\n",
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sentence_1 of catalog of sentences.\n",
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Wall Street Journal> \n",
      "\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "18 \n",
      "\n",
      "100676 \n",
      "\n",
      "12408 \n",
      "\n",
      "['craze', 'Somerset', 'Spiegel', 'till', 'uncharted', 'PORTING', 'deterioration', 'put', 'Uncertainty', 'Hutchinson']\n"
     ]
    }
   ],
   "source": [
    "# explore text 7\n",
    "print(text7,'\\n')\n",
    "# look at sentence_7 which is one sentence from text 7.\n",
    "print(sent7,'\\n')\n",
    "# lets get the lenghth of this sent_7.\n",
    "print(len(sent7),'\\n')\n",
    "# the length of the entire text_7.\n",
    "print(len(text7),'\\n')\n",
    "# So, what is the unique num of words\n",
    "print(len(set(text7)),'\\n')\n",
    "# getting the first 10 unique words.\n",
    "print(list(set(text7))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 4885, 'the': 4045, '.': 3828, 'of': 2319, 'to': 2164, 'a': 1878, 'in': 1572, 'and': 1511, '*-1': 1123, '0': 1099, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, if you want to find out the freq. of words.\n",
    "dist = FreqDist(text7)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the length of dist is equal to the num of unique words in text.\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'is', 'chairman']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# To get a list of words in dist (Vocabs):\n",
    "vocab1 = dist.keys()\n",
    "# put it into list because 'dict_keys' object is not subscriptable.\n",
    "print(list(vocab1)[5:20])\n",
    "# if i want to find how many times a particular word occurs.\n",
    "print(dist['four'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to know how many times a particular word occurs ,\n",
    "# and also have a condition on its length + num of occurrences.\n",
    "# Restriction on length to avoid words like `the` or `,` \n",
    "# thus get real frequent words.\n",
    "\n",
    "freqwords = [w for w in vocab1 if len(w)>5 and dist[w]>100] \n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Normalization and stemming.\n",
    "Means different forms of the same 'WORD', and we want them to be treated as one word instead of multiple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'listed', 'lists', 'listing', 'listings']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "# To do so,First, we need to lower() all of these versions.\n",
    "words1 = input1.lower().split()\n",
    "print(words1)\n",
    "# Then , bring up porterstemmer() to do this stemming task.\n",
    "# stem: means `Originate` in other word get the origin of the word.\n",
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Lemmatization.\n",
    "Slight variant of stemming. Which means the word that comes out be actually meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'rights', 'of'] \n",
      "\n",
      "['univers', 'declar', 'of', 'human', 'right', 'preambl', 'wherea', 'recognit', 'of', 'the', 'inher', 'digniti', 'and', 'of', 'the', 'equal', 'and', 'inalien', 'right', 'of']\n"
     ]
    }
   ],
   "source": [
    "# set the `Univ ersal Decleration of Human Rights`\n",
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(udhr[:20],'\\n')\n",
    "# lets apply stemming onto these bunch of words.\n",
    "print([porter.stem(t) for t in udhr[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see above, stemming did a good job on originate some words like `rights`etc. But, you might also have noticed that `univers`&`declar` aren't a valid word.\n",
    "\n",
    "**So, what solves this issue is Lemmatization, which means stemming, but resulting stems that are all valid words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'right', 'of']\n"
     ]
    }
   ],
   "source": [
    "# set the word net Lemmatizer \n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "print([WNlemma.lemmatize(t) for t in udhr[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](5.png)\n",
    "- you can see that all the stems{origins} are meaningful, but `Rights` is still like it was because it's capitalized. So , we need to `lower` all of them to make accurate lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['universal', 'declaration', 'of', 'human', 'right', 'preamble', 'whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'right', 'of']\n"
     ]
    }
   ],
   "source": [
    "# lower & lemmatize :\n",
    "lowered_list = [w.lower() for w in list(udhr[:20])]\n",
    "print([WNlemma.lemmatize(t) for t in lowered_list])\n",
    "\n",
    "# Now, `Rights` is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Tokenization.\n",
    "Recall splitting a setence into words / Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that just splitting on space didn't provide quite good job where it kept the dot with the word `bed.`, which is not appropriate at all.**\n",
    "\n",
    "- **The solution here : is Applying NLTK tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Children', 'should', \"n't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(text11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](6.png)\n",
    "**You will notice that shouldn't became `should` and this \"`n't`\" that stands for \"not\", and that is important in quite a few NLP task because you want to know negation = {contradiction or denial of S.M} here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Its important to know that there are some unique words like n apostrophe t that should also be separated and so on. `But there is even more fundamental question of, what is a sentence and how do you know sentence boundaries?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\n"
     ]
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "print(text12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So already, you know that a sentence can end with a full stop or a question mark or an exclamation mark and so on. But, not all full stops and sentences:\n",
    "\n",
    "- So for example, U dot S dot, that stands for US is just one word, has two full stops, but neither of them end the sentence. The same thing with $2.99.  That full stop is an indicator of a number but not end of a sentence.\n",
    "\n",
    "- The solution is :We could use NLTK's inbuilt sentence splitter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text12)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Advanced NLP tasks with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -  Part_of_speech (POS) Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "None \n",
      "\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "None \n",
      "\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# let's take a look the different `Tags` in English.\n",
    "print(nltk.help.upenn_tagset('MD'),'\\n')\n",
    "print(nltk.help.upenn_tagset('VB'),'\\n')\n",
    "print(nltk.help.upenn_tagset('DT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children shouldn't drink a sugary drink before bed. \n",
      "\n",
      "['Children', 'should', \"n't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, lets see how useful it can be in action.\n",
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "print(text11,'\\n')\n",
    "# 1st: tokenize\n",
    "tokens = nltk.word_tokenize(text11)\n",
    "print(tokens)\n",
    "# 2nd: run the post tagger.\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visiting', 'VBG'),\n",
       " ('aunts', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('nuisance', 'NN')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](9.png)\n",
    "**The debate here, is that you don't accurately know either it means visiting aunts as a verb you do or an ultimate noun, and here where the devil lies and the the ambiguity takes place.**\n",
    "\n",
    "**The is no room for variations or getting all options available in NLTK.pos_tag() , Thus it gives the most common form of it which is `VBG` stands for Gurand-verb.**\n",
    "\n",
    "Note : nuisance =  inconvenience or annoyance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Parsing Sentence Structure. \n",
    "![10](10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'loves', 'Bob'] \n",
      "\n",
      "[Tree('S', [Tree('NP', ['Alice']), Tree('VP', [Tree('V', ['loves']), Tree('NP', ['Bob'])])])] \n",
      "\n",
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "text15 = 'Alice loves Bob'\n",
    "text15_tokens = nltk.word_tokenize(text15)\n",
    "print(text15_tokens,'\\n')\n",
    "# make the grammer structure.\n",
    "grammer = nltk.CFG.fromstring(\"\"\"\n",
    "                              S -> NP VP\n",
    "                              VP -> V NP\n",
    "                              NP -> 'Alice' | 'Bob'\n",
    "                              V -> 'loves'\n",
    "                              \"\"\")\n",
    "parser = nltk.ChartParser(grammer)\n",
    "trees = parser.parse_all(text15_tokens)\n",
    "print(trees,'\\n')\n",
    "\n",
    "for tree in trees:\n",
    "    print(tree)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP', ['I']), Tree('VP', [Tree('VP', [Tree('V', ['saw']), Tree('NP', [Tree('Det', ['the']), Tree('N', ['man'])])]), Tree('PP', [Tree('P', ['with']), Tree('NP', [Tree('Det', ['a']), Tree('N', ['telescope'])])])])]), Tree('S', [Tree('NP', ['I']), Tree('VP', [Tree('V', ['saw']), Tree('NP', [Tree('Det', ['the']), Tree('N', ['man']), Tree('PP', [Tree('P', ['with']), Tree('NP', [Tree('Det', ['a']), Tree('N', ['telescope'])])])])])])] \n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (Det the) (N man)))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man) (PP (P with) (NP (Det a) (N telescope))))))\n"
     ]
    }
   ],
   "source": [
    "text16 = \"I saw the man with a telescope\"\n",
    "text16_tokens = nltk.word_tokenize(text16)\n",
    "\n",
    "grammer1 = nltk.CFG.fromstring(\"\"\"\n",
    "                            S -> NP VP\n",
    "                            PP -> P NP\n",
    "                            NP -> Det N | Det N PP | 'I'\n",
    "                            VP -> V NP | VP PP\n",
    "                            Det -> 'a' | 'the'\n",
    "                            N -> 'man' | 'telescope'\n",
    "                            V -> 'saw'\n",
    "                            P -> 'with'\n",
    "                              \"\"\")\n",
    "parser = nltk.ChartParser(grammer1)\n",
    "trees = parser.parse_all(text16_tokens)\n",
    "print(trees,'\\n')\n",
    "\n",
    "for tree in trees:\n",
    "    print(tree)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Now, we gave examples of simple grammars, and said we'll create a context for grammar out of it, but we can not do that every time. In fact, generating the grammar and generating grammar rules itself is a learning task that you could learn, and you need a lot of training data for that:\n",
    "- And a lot of manual effort and hours have gone into creating what is known as a `tree bank`, basically, a big collection of parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(sent7,'\\n')\n",
    "\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging and parsing ambiguity.\n",
    "![12](12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old man the boat \n",
      "\n",
      "['The', 'old', 'man', 'the', 'boat'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text18 = \"The old man the boat\"  # man[verb] = support, boost , use ,occupy\n",
    "print(text18,'\\n')\n",
    "\n",
    "text18_tokens = nltk.word_tokenize(text18)\n",
    "print(text18_tokens,'\\n')\n",
    "\n",
    "# apply post_tag: \n",
    "nltk.pos_tag(text18_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorless green ideas sleep furiously \n",
      "\n",
      "['Colorless', 'green', 'ideas', 'sleep', 'furiously'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Colorless', 'NNP'),\n",
       " ('green', 'JJ'),\n",
       " ('ideas', 'NNS'),\n",
       " ('sleep', 'VBP'),\n",
       " ('furiously', 'RB')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text19 = 'Colorless green ideas sleep furiously'  \n",
    "print(text19,'\\n')\n",
    "\n",
    "text19_tokens = nltk.word_tokenize(text19)\n",
    "print(text19_tokens,'\\n')\n",
    "\n",
    "# apply post_tag: \n",
    "nltk.pos_tag(text19_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![13](13.png)\n",
    "- **Next module, we're going to go into more detail about how do you train these models, how do you build a supervised method, and supervised technique, for these.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
