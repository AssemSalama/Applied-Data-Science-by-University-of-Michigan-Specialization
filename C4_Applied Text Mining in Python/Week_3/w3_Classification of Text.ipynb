{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)Text Classification.\n",
    "![1](1.png)\n",
    "![2](2.png)\n",
    "![3](3.png)\n",
    "![4](4.png)\n",
    "![5](5.png)\n",
    "![6](6.png)\n",
    "![7](7.png)\n",
    "![8](8.png)\n",
    "![9](9.png)\n",
    "![10](10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Identifying Features from Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](11.png)\n",
    "![12](12.png)\n",
    "- So, in some cases, you want to leave it as is, in some cases you want it lowercase, so how do you make that choice?\n",
    "- There are also issues about stemming and lemmatization. So for example, you don't want plurals to be different features. So you would want to lemmatize them or to stand them.\n",
    "![13](13.png)\n",
    "![14](14.png)\n",
    "![15](15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Naive Bayes Classifiers.\n",
    "- In this video we are going to talk about one of the classification algorithms called the Naïve Bayes classifier.\n",
    "![16](16.png)\n",
    "![17](17.png)\n",
    "![18](18.png)\n",
    "![19](19.png)\n",
    "![20](20.png)\n",
    "![21](21.png)\n",
    "\n",
    "- **So the Naïve Bayes classifier looks at this computation, looks at what is the probability of a class like computer science given the input as Python. And if computes is saying \"What is the chance that it was coming to science in general? What is the likelihood of it being computer science or the prior probability? And then what is the likelihood of seeing the word Python in documents that are computer science. The same thing with zoology.**\n",
    "\n",
    "\n",
    "- **So the probability of the class being zoology given Python is, what is a prior probability of the class being zoology without knowing any information? And then given that it is the zoology class document, what is the chance that you will see? What is the likelihood of seeing Python? And then you will say that if probability of y as CS given Python is higher than y as zoology, the label as zoology given Python then I'm going to call the label as computer science.** \n",
    "\n",
    "![22](22.png)\n",
    "\n",
    "\n",
    "- And so, the true label, or the predicted label I should say, y* is the y that maximizes probabilit of y given X. And then that computation it does not matter what the probability of X itself is. So what is the probability of seeing a query like Python. And you can remove it then because it does not matter, it doesn't change with the label assigned to it.\n",
    "\n",
    "\n",
    "- This in addition to what is called the Naïve assumption of Bayesian classification forms the Naïve Bayes classifier. And the Naïve assumption is that given the class label, the features themselves are independent of each other.\n",
    "\n",
    "![23](23.png)\n",
    "\n",
    "- you're going to say \"The predicted y is the y that maximizes probability of y. Probability of Python given y, and probability of download given y.\n",
    "\n",
    "\n",
    "- So for example, if it is zoology, you know that probability of zoology is low. People don't typically ask zoology queries. But then probability of Python given zoology is very high. However, probability of download given Python is very low. However, in the case of computer science, probability of computer science queries in general is somewhere in the middle. Probability of Python given computer sciences not up there but also significant. Whereas probability of download given computer science is very significant. And a product of all of the three makes computer science as to be the best predicted leap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "![24](24.png)\n",
    "\n",
    "You are training a naïve Bayes classifier, where the number of possible labels, |Y| = 3 and the dimension of the data element, |X| = 100, where every feature (dimension) is binary. How many parameters does the naïve Bayes classification model have?\n",
    "\n",
    "- |Y| +  2 x |X| +x|Y| = 603\n",
    "\n",
    "A naïve Bayes classifier has two kinds of parameters:\n",
    "- Pr(y) for every y in Y: so if |Y| = 3, there are three such parameters.\n",
    "\n",
    "-  Pr(x_i | y) for every binary feature x_i in X and y in Y. Specifically, for a particular feature x_1, the parameters are Pr(x_1 = 1 | y) and Pr(x_1 = 0 | y) for every y. So if |X| = 100 binary features and |Y| = 3, there are (2 x 100) x 3 = 600 such features\n",
    "\n",
    "Hence in all, there are 603 features.\n",
    "\n",
    "`Note` : that not all of these features areindependent. In particular, Pr(x_i = 0 | y) = 1 - Pr(x_i = 1 | y), for every x_i and y. So, there are only 300 independent parameters of this kind (as the other 300 parameters are just complements of these). Similarly, the sum of all prior probabilities Pr(y) should be 1. This means there are only 2 independent prior probabilities. In all, for this example, there are 302 independent parameters.\n",
    "\n",
    "![25](25.png)\n",
    "![26](26.png)\n",
    "![27](27.png)\n",
    "![28](28.png)\n",
    "\n",
    "**- So for example, the fact that you have \"White House\" as two words but together having a significantly different meaning. If you see the word \"White\" with a capital W, the chance that it is followed by \"House\" is very high. So these two features of \"White\" and \"House\" are not really independent of each other. But Naïve Bayes models kind of ignored that.**\n",
    "\n",
    "**- They say that given the class label, I'm assuming all features are independent because the math on how you can compute the probabilities becomes easy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Naive Bayes Variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![29](29.png)\n",
    "**So, suppose you have a piece of text, a document, and you are finding out what are all the words that were used in this model. That would be called a bag-of-words model. And if you just use the words, whether they were present or not, then that is a Bernoulli distribution for each feature:**\n",
    "\n",
    "- So, it becomes a multivariate Bernoulli when you're talking about it for all the words. But if you say that the number of times a particular word occurs is important, so for example, if the statement is to be or not to be, and you want to somehow say that the word to occur twice, the word be occur twice, the word or occur just once and so on, you want to somehow keep track of what was the frequency of each of these words.\n",
    "\n",
    "**And then, if you want to give more importance to more rare words, then you would add on something called a term frequency, inverse document frequency weighting. So you don't, not only give importance to the frequency, but say how common is this word in the entire collection, and that's what the idea of weighting comes from:**\n",
    "\n",
    "- So for example, the word THE is very common, it occurs on almost every sentence, it occurs in every document, so it is not very informative. But if it is the word, like, SIGNIFICANT, it is significant because it's not gonna be occurring in every document. So, you want to give a higher importance to a document that has this word significant as compared to the word the, and that kind of variation in weighting is possible when you're doing a multinomial Naïve Bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Support Vector Machines.\n",
    "![30](30.png)\n",
    "![31](31.png)\n",
    "![32](32.png)\n",
    "![33](33.png)\n",
    "![34](34.png)\n",
    "![35](35.png)\n",
    "![36](36.png)\n",
    "- Note : In this case we're gonna use `support vector machine` method.\n",
    "![37](37.png)\n",
    "![38](38.png)\n",
    "![39](39.png)\n",
    "![40](40.png)\n",
    "![41](41.png)\n",
    "- Note : Means n^2\n",
    "![42](42.png)\n",
    "- The defualt value =1 but, you can change it as you want.\n",
    "![43](43.png)\n",
    "- For example, if you want a particular class, let's say spam or not spam, and you know that the spams are usually like 80% of e-mails somebody gets. So then he would want, that because it's just such a skewed distribution where one of the classes 80% and the other classes 20% you would want to give different weight to these two classes. \n",
    "![44](44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Learning Text Classifiers in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![45](45.png)\n",
    "![46](46.png)\n",
    "![47](47.png)\n",
    "![48](48.png)\n",
    "![49](49.png)\n",
    "- There are two ways you could do model selection. One is keeping some part of printing the label data set separate as the `hold out` data. And the other option is `cross-validation`.\n",
    "\n",
    "### `hold out` data:\n",
    "![50](50.png)\n",
    "###  `cross-validation` approach:\n",
    "![51](51.png)\n",
    "\n",
    "![52](52.png)\n",
    "![53](53.png)\n",
    "![54](54.png)\n",
    "![55](55.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Case Study - Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data\\Amazon_Unlocked_Mobile.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- We'll focus on `Rating` and `Reviews` most of the time to make our sentiment Analysis.**\n",
    "\n",
    "## 1. Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the rating is out of which number:\n",
    "df['Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 413840 entries, 0 to 413839\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Product Name  413840 non-null  object \n",
      " 1   Brand Name    348669 non-null  object \n",
      " 2   Price         407907 non-null  float64\n",
      " 3   Rating        413840 non-null  int64  \n",
      " 4   Reviews       413778 non-null  object \n",
      " 5   Review Votes  401544 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 18.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# find out the nulls and the d-type of each feature.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we need to droop those rows with NaNs.\n",
    "2. remove `3` kind of rating as it's `neutral` not good nor bad.\n",
    "3. create a column called `positively rated` its value is `1 for[5,4]`rating and `0 for[1,2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>The charging port was loose. I got that solder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "6  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2       5                                       Very pleased           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "5       1  I already had a phone with problems... I know ...           1.0   \n",
       "6       2  The charging port was loose. I got that solder...           0.0   \n",
       "\n",
       "   Positively Rated  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 0  \n",
       "6                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1:\n",
    "df.dropna(inplace=True)\n",
    "# 2:\n",
    "df = df[df['Rating'] !=3]\n",
    "# 3:\n",
    "x =[4,5]\n",
    "df['Positively Rated'] = np.where( df['Rating']>3 ,1,0)\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482686025879323"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the average of positively rated col to check \n",
    "# if there is a class imbalance.\n",
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  As you can see Most ratings are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting our data for model training & testing:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],\n",
    "                                                 df['Positively Rated'],\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- X_tarin first entry : \n",
      " I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\n",
      "\n",
      "\n",
      "- X_train shape (num of review in it) : (231207,)\n"
     ]
    }
   ],
   "source": [
    "# lets look at X_train 1st entry:\n",
    "print('- X_tarin first entry : \\n', X_train[0])\n",
    "print('\\n\\n- X_train shape (num of review in it) :', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CountVectorizer.\n",
    "**Looking at X_train, we can see we have a series of over 231,000 reviews or documents. We'll need to convert these into a numeric representation that scikit-learn can use. The bag-of-words approach is simple and commonly used way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs. CountVectorizer allows us to use the bag-of-words approach by converting a collection of text documents into a matrix of token counts:**\n",
    "\n",
    "- First, we instantiate the CountVectorizer and fit it to our training data. Fitting the CountVectorizer consists of the tokenization of the trained data and building of the vocabulary.\n",
    "\n",
    "- Fitting the CountVectorizer tokenizes each document by finding all sequences of characters of at least two letters or numbers separated by word boundaries. Converts everything to lowercase and builds a vocabulary using these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '4less',\n",
       " 'adr6275',\n",
       " 'assignment',\n",
       " 'blazingly',\n",
       " 'cassettes',\n",
       " 'condishion',\n",
       " 'debi',\n",
       " 'dollarsshipping',\n",
       " 'esteem',\n",
       " 'flashy',\n",
       " 'gorila',\n",
       " 'human',\n",
       " 'irullu',\n",
       " 'like',\n",
       " 'microsaudered',\n",
       " 'nightmarish',\n",
       " 'p770',\n",
       " 'poori',\n",
       " 'quirky',\n",
       " 'responseive',\n",
       " 'send',\n",
       " 'sos',\n",
       " 'synch',\n",
       " 'trace',\n",
       " 'utiles',\n",
       " 'withstanding']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get these vocabulary using `get_feature_names()`\n",
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Brand new slicing technique**\n",
    "- Lst[ Initial : End : IndexJump ] : If Lst is a list, then the above expression returns the portion of the list from index Initial to index End, at a step size IndexJump.\n",
    "\n",
    "source: https://www.geeksforgeeks.org/python-list-slicing/#:~:text=In%20Python%2C%20list%20slicing%20is%20a%20common%20practice,use%20the%20simple%20slicing%20operator%20i.e.%20colon%20%28%3A%29\n",
    "\n",
    "Looking at every 2,000th feature, we can get a small sense of what the vocabulary looks like. We can see it looks pretty messy, including words with numbers as well as misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53216"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, let's give a look at the num of vocabs(feature) we are dealing with.\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Next,`: we use the transform method to transform the documents in X_train to a document term matrix, giving us the bag-of-word representation of X_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This representation is stored in a SciPy sparse matrix, where each row corresponds to a document and each column a word from our training vocabulary.\n",
    "\n",
    "\n",
    "2. The entries in this matrix are the number of times each word appears in each document.\n",
    "\n",
    "\n",
    "3. Because the number of words in the vocabulary is so much larger than the number of words that might appear in a single review, most entries of this matrix are zero.\n",
    "\n",
    "\n",
    "**Now let's use this feature matrix X_ train_ vectorized to train our model. We'll use `LogisticRegression`, which works well for `high dimensional sparse data`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Assem Salama\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Train the model:\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC : 0.9206104942478054\n"
     ]
    }
   ],
   "source": [
    "# predict & evaluate the model:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print(' AUC :', roc_auc_score(y_test ,predictions))\n",
    "\n",
    "# Note that any words in X_test that didn't appear in X_train will\n",
    "# just be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the coefficients from our model:\n",
    "\n",
    "- Sorting them and looking at the ten smallest and ten largest coefficients, we can see the model has connected words like worst, worthless and junk to negative reviews. And words like excellent, loves, and amazing to positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., '索尼大法好', '还不错', '좋군'], dtype='<U567')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the feature names (vocabs) as numpy array:\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coeffeicients: [[-0.25565377  0.19706971  0.02266777 ...  0.00115706  0.15414915\n",
      "   0.01223158]]\n",
      "model coeffeicients shape : (1, 53216)\n",
      "\n",
      "\n",
      "model coef as vector :  [-0.25565377  0.19706971  0.02266777 ...  0.00115706  0.15414915\n",
      "  0.01223158]\n",
      "model coef as vector shape : (53216,)\n",
      "\n",
      "Smallest Coefs:\n",
      "['worst' 'garbage' 'junk' 'unusable' 'false' 'worthless' 'useless'\n",
      " 'crashing' 'disappointing' 'awful']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' 'exelente' 'loving' 'loves' 'perfecto' 'excellent'\n",
      " 'complaints' 'awesome' 'buen']\n"
     ]
    }
   ],
   "source": [
    "# the purpose of these lines of codes are to inform you\n",
    "# why we will put [0] at sorted indexes:\n",
    "# 1--`model.coef_` came to us as np array into a shape of (1, 53216).\n",
    "print('model coeffeicients:',model.coef_)\n",
    "print('model coeffeicients shape :',model.coef_.shape)\n",
    "\n",
    "# 2--But this is not appropriate where we want it as a `vector`\n",
    "print('\\n\\nmodel coef as vector : ',model.coef_[0])\n",
    "print('model coef as vector shape :',model.coef_[0].shape)\n",
    "\n",
    "\n",
    "# Our Aim: is to Sort the coefficients\n",
    "# (weights of each word in the features names) from the model:\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('\\nSmallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`argsort()` : Sort the original array `ascendingly(1-10)` then gives an array of indexes in this order.**\n",
    "\n",
    "Note : \n",
    "- coeffecient is correlated with features(vocabs) which means in the same order. So, if i can sort the coeffecient , i'll be able to identify the most & lowest important features(vocabs).\n",
    "\n",
    "source: https://www.geeksforgeeks.org/numpy-argsort-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tfidf\n",
    "A different approach, which allows us to rescale features called tf–idf. Tf–idf, or Term frequency-inverse document frequency, allows us to weight terms based on how important they are to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17951"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a \n",
    "# minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Assem Salama\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC :  0.9265848398605042\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# train\n",
    "model = LogisticRegression().fit(X_train_vectorized,y_train)\n",
    "\n",
    "# predict & evaluate\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC : ' ,roc_auc_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['commenter' 'pthalo' 'warmness' 'storageso' 'aggregration' '1300'\n",
      " '625nits' 'a10' 'submarket' 'brawns']\n",
      "\n",
      "Largest tfidf: \n",
      "['defective' 'batteries' 'gooood' 'epic' 'luis' 'goood' 'basico'\n",
      " 'aceptable' 'problems' 'excellant']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of features with the smallest tf–idf either commonly appeared across all reviews or only appeared rarely in very long reviews.\n",
    "- List of features with the largest tf–idf contains words which appeared frequently in a review, but did not appear commonly across all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'worst' 'useless' 'disappointed' 'terrible' 'return' 'waste' 'poor'\n",
      " 'horrible' 'doesn']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'excellent' 'perfect' 'amazing' 'awesome' 'perfectly'\n",
      " 'easy' 'best' 'loves']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- The `Problem` with this Approach -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are treated the same by our current model as negative \n",
    "# reviews because words order is disregarded.\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. n-grams\n",
    "\n",
    "### ----- The `Solution` of the problem above------\n",
    "One way we can add some context is by adding sequences of word features known as n-grams. For example, bigrams, which count pairs of adjacent words, could give us features such as `is working` versus `not working`. And trigrams, which give us triplets of adjacent words, could give us features such as `not an issue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198917"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams(1 word) and 2-grams(2 words).\n",
    "# Note: you can use `TfidfVectorizer` as well,\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Assem Salama\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9588716211702142\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'not happy' 'not worth' 'junk' 'worst' 'not satisfied'\n",
      " 'garbage' 'not good' 'defective' 'terrible']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' 'excellent' 'not bad' 'exelente' 'perfect'\n",
      " 'awesome' 'no problems' 'no issues' 'perfecto']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The vectorizers we saw in this tutorial are very flexible and also support tasks such as removing stop words or limitization. So be sure to check the documentation for more info.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
