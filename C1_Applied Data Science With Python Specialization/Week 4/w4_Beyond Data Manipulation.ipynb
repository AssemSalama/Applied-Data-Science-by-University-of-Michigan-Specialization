{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import our usual libararies.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# our new geust \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When we do hypothesis testing, we actually have two statements of interest:__\n",
    "\n",
    "- the first is our actual explanation, which we call the alternative hypothesis.\n",
    "\n",
    "- the second is that the explanation we have is not sufficient, and we call this the null hypothesis.\n",
    "\n",
    "``Our actual testing method is to determine whether the null hypothesis is true or not. If we find that there is a difference between groups, then we can reject the null hypothesis and we accept our alternative.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "2  2016-01-17 16:24:42.765000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's examine the idea by example:\n",
    "df = pd.read_csv('data/grades.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2315, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the shape of df\n",
    "# we have 2315 student and 13 column for 'ass_grade' & 'ass_submission_date'\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll segment those students according to the submission date:\n",
    "- So, for those who finished the 1st ass by the end of DEC 2015  are`early finishers`.\n",
    "- those who finished it sometimes after that are `late finishers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3217BE3F-E4B0-C3B6-9F64-462456819CE4</td>\n",
       "      <td>87.498744</td>\n",
       "      <td>2016-03-05 11:05:25.408</td>\n",
       "      <td>69.998995</td>\n",
       "      <td>2016-03-09 07:29:52.405000000</td>\n",
       "      <td>55.999196</td>\n",
       "      <td>2016-03-16 22:31:24.316000000</td>\n",
       "      <td>50.399276</td>\n",
       "      <td>2016-03-18 07:19:26.032000000</td>\n",
       "      <td>45.359349</td>\n",
       "      <td>2016-03-19 10:35:41.869000000</td>\n",
       "      <td>45.359349</td>\n",
       "      <td>2016-03-23 14:02:00.987000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "6  3217BE3F-E4B0-C3B6-9F64-462456819CE4          87.498744   \n",
       "\n",
       "   assignment1_submission  assignment2_grade         assignment2_submission  \\\n",
       "2 2016-01-09 05:36:02.389          85.512541  2016-01-09 06:39:44.416000000   \n",
       "3 2016-04-30 06:50:39.801          68.824532  2016-04-30 17:20:38.727000000   \n",
       "6 2016-03-05 11:05:25.408          69.998995  2016-03-09 07:29:52.405000000   \n",
       "\n",
       "   assignment3_grade         assignment3_submission  assignment4_grade  \\\n",
       "2          68.410033  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3          61.942079  2016-05-12 07:47:16.326000000          49.553663   \n",
       "6          55.999196  2016-03-16 22:31:24.316000000          50.399276   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "6  2016-03-18 07:19:26.032000000          45.359349   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "6  2016-03-19 10:35:41.869000000          45.359349   \n",
       "\n",
       "          assignment6_submission  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "6  2016-03-23 14:02:00.987000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's make the `assignment1_submission` col to DateTime.\n",
    "df['assignment1_submission'] = pd.to_datetime(df['assignment1_submission'])\n",
    "\n",
    "# then we'll segment according to this column \n",
    "early_finishers = df[df['assignment1_submission'] < '2016']\n",
    "\n",
    "# SO, whithout following the same approach that we used to it lets try out \n",
    "# another one: Since there are no students can be in both dfs , we'll include\n",
    "# all students in 'df' that are not in 'early_finishers' using the defualt \n",
    "# index col.\n",
    "late_finishers = df[~df.index.isin(early_finishers.index)]\n",
    "late_finishers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other ways to do this:\n",
    "- For instance, you could just copy and paste the first projection and change the sign from less than to greater than or equal to. This is ok, but if you decide you want to change the date down the road you have to remember to change it in two places.\n",
    "____\n",
    "- You could also do a join of the dataframe df with early_finishers - if you do a left join you only keep the items in the left dataframe, so this would have been a good answer.\n",
    "____\n",
    "- You also could have written a function that determines if someone is early or late, and then called .apply() on the dataframe and added a new column to the dataframe. This is a pretty reasonable answer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.94728457024303\n",
      "74.0450648477065\n"
     ]
    }
   ],
   "source": [
    "# Now, lets examine if there is a higher avg grades for early over late(Ass1)\n",
    "print(early_finishers['assignment1_grade'].mean())\n",
    "print(late_finishers['assignment1_grade'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, these looks pretty similar. But,are they really the same?. So, this is where t-test comes in. it allows us to form the alternative hypothesis `these are different` as well as the null hypothesis `these are the same`.\n",
    "\n",
    "- when we're doing hypothesis testing, we have to choose the signeficant level as a threeshold we're willing to accept which called alpha. lets use 5% (0.05) ,this is commonly used percentage but is may be wrong.\n",
    "\n",
    "The SciPy library contains a number of different statistical tests and forms a basis for hypothesis testing in Python:\n",
    "- we're going to use the `ttest_ind()` function which does an independent t-test (meaning the populations are not related to one another).\n",
    "___\n",
    "- The result of ttest_ind() are the t-statistic and a `p-value`. It's this latter value, the probability, which is most important to us, as `it indicates the chance (between 0 and 1) of our null hypothesis being True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.322354085372139, pvalue=0.1861810110171455)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's bring in our ttest_ind() from scipy and pass in our two groups.\n",
    "from scipy.stats import ttest_ind \n",
    "ttest_ind(early_finishers['assignment1_grade'], late_finishers['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the pvalue = 0.18 and this exceeds our alpha(the threesold to accept the null) = 0.05.\n",
    "- which means that we cannot reject the null that inform us that the 2 groups averege Ass1 grades are the same.\n",
    "- we don't have enough certainty in our evidence to come with the contrary.but this doesn't mean that we've proven that the 2 pops are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=1.2514717608216366, pvalue=0.2108889627004424)\n",
      "Ttest_indResult(statistic=1.6133726558705392, pvalue=0.10679998102227865)\n",
      "Ttest_indResult(statistic=0.049671157386456125, pvalue=0.960388729789337)\n",
      "Ttest_indResult(statistic=-0.05279315545404755, pvalue=0.9579012739746492)\n",
      "Ttest_indResult(statistic=-0.11609743352612056, pvalue=0.9075854011989656)\n"
     ]
    }
   ],
   "source": [
    "# why don't we check all the Assigments for both groups.\n",
    "print(ttest_ind(early_finishers['assignment2_grade'], late_finishers['assignment2_grade']))\n",
    "print(ttest_ind(early_finishers['assignment3_grade'], late_finishers['assignment3_grade']))\n",
    "print(ttest_ind(early_finishers['assignment4_grade'], late_finishers['assignment4_grade']))\n",
    "print(ttest_ind(early_finishers['assignment5_grade'], late_finishers['assignment5_grade']))\n",
    "print(ttest_ind(early_finishers['assignment6_grade'], late_finishers['assignment6_grade']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it looks like in this data we do not have enough evidence to suggest the populations differ with respect to grade.Let's take a look at those p-values for a moment though, because they are saying things that can inform experimental design down the road:\n",
    "\n",
    "- For instance, one of the assignments, assignment 3, has a p-value around 0.1. This means that if we accepted a level of chance similarity of 11% this would have been considered statistically significant.\n",
    "\n",
    "As a research, this would suggest to me that there is something here\n",
    "worth considering following up on:\n",
    "\n",
    "- For instance, if we had a small number of participants (we don't) or if there was something unique about this assignment as it relates to our experiment (whatever it was) then there may be follow up experiments we could run.\n",
    "\n",
    "P-values have come under fire recently for being insuficient for telling us enough about the interactions which are happening, and two other techniques, confidence intervalues and bayesian analyses, are being used\n",
    "more regularly:\n",
    "- One issue with p-values is that as you run more tests you are likely to get a value which is statistically significant just by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894592</td>\n",
       "      <td>0.049842</td>\n",
       "      <td>0.253985</td>\n",
       "      <td>0.125167</td>\n",
       "      <td>0.697074</td>\n",
       "      <td>0.381103</td>\n",
       "      <td>0.405503</td>\n",
       "      <td>0.603949</td>\n",
       "      <td>0.622662</td>\n",
       "      <td>0.500422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461025</td>\n",
       "      <td>0.275951</td>\n",
       "      <td>0.879728</td>\n",
       "      <td>0.635666</td>\n",
       "      <td>0.930484</td>\n",
       "      <td>0.294695</td>\n",
       "      <td>0.71195</td>\n",
       "      <td>0.752545</td>\n",
       "      <td>0.574683</td>\n",
       "      <td>0.575228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.894592  0.049842  0.253985  0.125167  0.697074  0.381103  0.405503   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.603949  0.622662  0.500422  ...  0.461025  0.275951  0.879728  0.635666   \n",
       "\n",
       "         94        95       96        97        98        99  \n",
       "0  0.930484  0.294695  0.71195  0.752545  0.574683  0.575228  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see a simulation of this,creating 2 df of 100 column each of them\n",
    "# with 100 numbers.\n",
    "df1 = pd.DataFrame([np.random.random(100) for x in range(100)])\n",
    "df2 = pd.DataFrame([np.random.random(100) for x in range(100)])\n",
    "\n",
    "# list comprehention says that i want a list that contains 100 random numbers\n",
    "# and iterate this process 100 times to get 100 lists each of them has 100\n",
    "# random number,Then use these arrays(lists) to create a dataFrame.  \n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these two DataFrames the same? Maybe a better question is, for a given row inside of df1, is it the same as the row inside df2? :\n",
    "\n",
    "- Let's say our critical value is 0.1, or alpha of 10%. And we're going to compare each column in df1 to the same numbered column in df2. And we'll report when the p-value isn't less than 10%, which means that we have sufficient evidence to say that the columns are different(NULL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col 8 is statistically significant at alpha 0.1 with pval 0.011201275622828858\n",
      "col 14 is statistically significant at alpha 0.1 with pval 0.05892081458812326\n",
      "col 16 is statistically significant at alpha 0.1 with pval 0.030765564152336108\n",
      "col 31 is statistically significant at alpha 0.1 with pval 0.045815940651396984\n",
      "col 40 is statistically significant at alpha 0.1 with pval 0.01008519508643523\n",
      "col 41 is statistically significant at alpha 0.1 with pval 0.0327183792538498\n",
      "col 44 is statistically significant at alpha 0.1 with pval 0.02750254769128424\n",
      "col 47 is statistically significant at alpha 0.1 with pval 0.09794875112250545\n",
      "col 48 is statistically significant at alpha 0.1 with pval 0.07477654456249958\n",
      "col 74 is statistically significant at alpha 0.1 with pval 0.06601385578943071\n",
      "col 75 is statistically significant at alpha 0.1 with pval 0.09918760239382138\n",
      "col 82 is statistically significant at alpha 0.1 with pval 0.043728057724814175\n",
      "col 87 is statistically significant at alpha 0.1 with pval 0.06613659537041422\n",
      "col 90 is statistically significant at alpha 0.1 with pval 0.06558157210707127\n",
      "\t\n",
      "Total number of columns to reject the NuLL 14, which is 14.000000000000002%\n"
     ]
    }
   ],
   "source": [
    "# lets create a function to do this report:\n",
    "def test_columns(alpha = 0.1):\n",
    "    total_num_col_to_reject_the_null = 0\n",
    "    # keep in mind that the 2 dfs have the same column index so i'll iterate\n",
    "    # over the coulmns of both using one for loop.\n",
    "    for col in df1.columns:\n",
    "        # As ttest_ind() returns 2 values lets do tuple unpacking.\n",
    "        teststat, pval = ttest_ind(df1[col], df2[col])\n",
    "        if pval <= alpha :\n",
    "            print('col {} is statistically significant at alpha {} with pval {}'. format(col,alpha,pval))\n",
    "            total_num_col_to_reject_the_null+=1\n",
    "    print('\\t') \n",
    "    print('Total number of columns to reject the NuLL {}, which is {}%'.format(total_num_col_to_reject_the_null,\n",
    "                                                                                                        total_num_col_to_reject_the_null/len(df1.columns)*100))\n",
    "test_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so we see that there are a bunch of columns that are different! In fact, that number looks a lot like the alpha value we chose. So what's going on - shouldn't all of the columns be the same?\n",
    "\n",
    "- `Remember that all the ttest does is check if two sets are similar given some level of confidence,` in our case, 10%.\n",
    "- The more random comparisons you do, the more will just happen to be the same by chance. In this example, we checked 100 columns, so we would expect there to be roughly 10 of them if our alpha was 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col 8 is statistically significant at alpha 0.05 with pval 0.011201275622828858\n",
      "col 16 is statistically significant at alpha 0.05 with pval 0.030765564152336108\n",
      "col 31 is statistically significant at alpha 0.05 with pval 0.045815940651396984\n",
      "col 40 is statistically significant at alpha 0.05 with pval 0.01008519508643523\n",
      "col 41 is statistically significant at alpha 0.05 with pval 0.0327183792538498\n",
      "col 44 is statistically significant at alpha 0.05 with pval 0.02750254769128424\n",
      "col 82 is statistically significant at alpha 0.05 with pval 0.043728057724814175\n",
      "\t\n",
      "Total number of columns to reject the NuLL 7, which is 7.000000000000001%\n"
     ]
    }
   ],
   "source": [
    "# what if we change the alpha:\n",
    "test_columns(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, keep this in mind when you are doing statistical tests like the t-test which has a p-value. Understand that this p-value isn't magic:\n",
    "- it's just a threshold for you when reporting results and trying to answer\n",
    "  your hypothesis.\n",
    "- What's a reasonable threshold? Depends on your question, and you need to engage domain experts to better understand what they would consider significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col 0 is statistically significant at alpha 0.1 with pval 0.003027205050417967\n",
      "col 1 is statistically significant at alpha 0.1 with pval 0.0014409394779935007\n",
      "col 2 is statistically significant at alpha 0.1 with pval 1.0838195758967829e-06\n",
      "col 3 is statistically significant at alpha 0.1 with pval 1.5260901684106276e-05\n",
      "col 4 is statistically significant at alpha 0.1 with pval 0.00022331668791731346\n",
      "col 5 is statistically significant at alpha 0.1 with pval 5.608306497295161e-06\n",
      "col 6 is statistically significant at alpha 0.1 with pval 0.0005365096125164064\n",
      "col 7 is statistically significant at alpha 0.1 with pval 0.00033698346183002063\n",
      "col 8 is statistically significant at alpha 0.1 with pval 0.0003463053453323207\n",
      "col 9 is statistically significant at alpha 0.1 with pval 0.001075109014313189\n",
      "col 10 is statistically significant at alpha 0.1 with pval 0.0007519321497065969\n",
      "col 11 is statistically significant at alpha 0.1 with pval 9.07153817408264e-06\n",
      "col 12 is statistically significant at alpha 0.1 with pval 0.0003382010712416859\n",
      "col 13 is statistically significant at alpha 0.1 with pval 0.00012990201966272755\n",
      "col 14 is statistically significant at alpha 0.1 with pval 0.060562614712349014\n",
      "col 15 is statistically significant at alpha 0.1 with pval 0.0035556097465065336\n",
      "col 16 is statistically significant at alpha 0.1 with pval 0.00017194961697101162\n",
      "col 17 is statistically significant at alpha 0.1 with pval 0.021445824561379754\n",
      "col 18 is statistically significant at alpha 0.1 with pval 2.3532934450176276e-05\n",
      "col 19 is statistically significant at alpha 0.1 with pval 0.0002206002262217297\n",
      "col 20 is statistically significant at alpha 0.1 with pval 0.0017485896761106929\n",
      "col 21 is statistically significant at alpha 0.1 with pval 0.0001417225411518568\n",
      "col 22 is statistically significant at alpha 0.1 with pval 2.1040521559054204e-05\n",
      "col 23 is statistically significant at alpha 0.1 with pval 0.01741339906454099\n",
      "col 24 is statistically significant at alpha 0.1 with pval 0.0004886316068035163\n",
      "col 25 is statistically significant at alpha 0.1 with pval 0.0018736794668131468\n",
      "col 26 is statistically significant at alpha 0.1 with pval 0.0003380798884427694\n",
      "col 27 is statistically significant at alpha 0.1 with pval 0.00047141662801929873\n",
      "col 28 is statistically significant at alpha 0.1 with pval 8.069914093785597e-06\n",
      "col 29 is statistically significant at alpha 0.1 with pval 0.0019194984343738012\n",
      "col 30 is statistically significant at alpha 0.1 with pval 0.00030459901109088056\n",
      "col 31 is statistically significant at alpha 0.1 with pval 0.00042334215161803773\n",
      "col 32 is statistically significant at alpha 0.1 with pval 0.02312931078018878\n",
      "col 33 is statistically significant at alpha 0.1 with pval 0.00789403994588463\n",
      "col 34 is statistically significant at alpha 0.1 with pval 0.0016354672576879721\n",
      "col 35 is statistically significant at alpha 0.1 with pval 1.1878977894608406e-06\n",
      "col 36 is statistically significant at alpha 0.1 with pval 0.0024301092091271693\n",
      "col 37 is statistically significant at alpha 0.1 with pval 0.014417715455365368\n",
      "col 38 is statistically significant at alpha 0.1 with pval 1.1563258610643362e-05\n",
      "col 39 is statistically significant at alpha 0.1 with pval 0.0002088810407932143\n",
      "col 40 is statistically significant at alpha 0.1 with pval 0.0009675654790752404\n",
      "col 41 is statistically significant at alpha 0.1 with pval 0.0012931710161958995\n",
      "col 42 is statistically significant at alpha 0.1 with pval 3.1663043881390476e-06\n",
      "col 43 is statistically significant at alpha 0.1 with pval 0.002100665307013185\n",
      "col 44 is statistically significant at alpha 0.1 with pval 0.009941705706246072\n",
      "col 45 is statistically significant at alpha 0.1 with pval 0.041425169638769334\n",
      "col 46 is statistically significant at alpha 0.1 with pval 0.007363364225062622\n",
      "col 47 is statistically significant at alpha 0.1 with pval 0.0038565773632441435\n",
      "col 48 is statistically significant at alpha 0.1 with pval 0.00010928124012620405\n",
      "col 49 is statistically significant at alpha 0.1 with pval 8.906661140478694e-06\n",
      "col 50 is statistically significant at alpha 0.1 with pval 0.04309887756403323\n",
      "col 51 is statistically significant at alpha 0.1 with pval 0.029645752481826552\n",
      "col 52 is statistically significant at alpha 0.1 with pval 0.014291721365838743\n",
      "col 53 is statistically significant at alpha 0.1 with pval 0.00012352961511348734\n",
      "col 54 is statistically significant at alpha 0.1 with pval 5.696325891799853e-05\n",
      "col 55 is statistically significant at alpha 0.1 with pval 0.00013563300749515222\n",
      "col 56 is statistically significant at alpha 0.1 with pval 0.0006871763768287742\n",
      "col 57 is statistically significant at alpha 0.1 with pval 0.00011289328762707434\n",
      "col 58 is statistically significant at alpha 0.1 with pval 0.0029342559449582056\n",
      "col 59 is statistically significant at alpha 0.1 with pval 0.0003856154499410264\n",
      "col 60 is statistically significant at alpha 0.1 with pval 2.709359205064025e-05\n",
      "col 61 is statistically significant at alpha 0.1 with pval 0.0008956972344661085\n",
      "col 62 is statistically significant at alpha 0.1 with pval 0.0007204812553615371\n",
      "col 63 is statistically significant at alpha 0.1 with pval 0.0021013949127494515\n",
      "col 64 is statistically significant at alpha 0.1 with pval 0.002422507083935409\n",
      "col 65 is statistically significant at alpha 0.1 with pval 3.948137027709044e-05\n",
      "col 66 is statistically significant at alpha 0.1 with pval 2.3989775083751128e-05\n",
      "col 67 is statistically significant at alpha 0.1 with pval 2.782016389420975e-05\n",
      "col 68 is statistically significant at alpha 0.1 with pval 0.007152431664289219\n",
      "col 69 is statistically significant at alpha 0.1 with pval 0.00015434492592951408\n",
      "col 70 is statistically significant at alpha 0.1 with pval 1.0141924415833567e-05\n",
      "col 71 is statistically significant at alpha 0.1 with pval 0.0027515136487062915\n",
      "col 72 is statistically significant at alpha 0.1 with pval 0.017202008925633103\n",
      "col 73 is statistically significant at alpha 0.1 with pval 0.002656310633320428\n",
      "col 74 is statistically significant at alpha 0.1 with pval 0.01635491369950556\n",
      "col 75 is statistically significant at alpha 0.1 with pval 0.004109913099663924\n",
      "col 76 is statistically significant at alpha 0.1 with pval 0.0001243808991755529\n",
      "col 77 is statistically significant at alpha 0.1 with pval 0.00030265959910602487\n",
      "col 78 is statistically significant at alpha 0.1 with pval 5.2475631988802106e-05\n",
      "col 79 is statistically significant at alpha 0.1 with pval 0.003334919502411368\n",
      "col 80 is statistically significant at alpha 0.1 with pval 3.146759229583596e-05\n",
      "col 81 is statistically significant at alpha 0.1 with pval 0.00017500567510531017\n",
      "col 82 is statistically significant at alpha 0.1 with pval 0.00011586879228405662\n",
      "col 83 is statistically significant at alpha 0.1 with pval 0.010331146407241736\n",
      "col 84 is statistically significant at alpha 0.1 with pval 4.316576064283496e-05\n",
      "col 85 is statistically significant at alpha 0.1 with pval 0.00021650999653927768\n",
      "col 86 is statistically significant at alpha 0.1 with pval 6.355469539121307e-05\n",
      "col 87 is statistically significant at alpha 0.1 with pval 0.0001901801676723067\n",
      "col 88 is statistically significant at alpha 0.1 with pval 6.108930407667028e-06\n",
      "col 89 is statistically significant at alpha 0.1 with pval 0.0015199759247217295\n",
      "col 90 is statistically significant at alpha 0.1 with pval 0.0002764462055158236\n",
      "col 91 is statistically significant at alpha 0.1 with pval 9.84632361297884e-05\n",
      "col 92 is statistically significant at alpha 0.1 with pval 0.00010398639184903873\n",
      "col 93 is statistically significant at alpha 0.1 with pval 0.0003516073395396443\n",
      "col 94 is statistically significant at alpha 0.1 with pval 3.957239617163897e-05\n",
      "col 95 is statistically significant at alpha 0.1 with pval 0.00668546743293844\n",
      "col 96 is statistically significant at alpha 0.1 with pval 0.006853336594842049\n",
      "col 97 is statistically significant at alpha 0.1 with pval 0.0005207821382432746\n",
      "col 98 is statistically significant at alpha 0.1 with pval 3.518816290414113e-06\n",
      "col 99 is statistically significant at alpha 0.1 with pval 0.00539912430639431\n",
      "\t\n",
      "Total number of columns to reject the NuLL 100, which is 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Just for fun, lets recreate that second dataframe using a non-normal distribution, I'll arbitrarily chose\n",
    "# chi squared\n",
    "df2=pd.DataFrame([np.random.chisquare(df=1,size=100) for x in range(100)])\n",
    "test_columns()\n",
    "\n",
    "# now we van see that most of the columns are statlly. significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we've discussed just some of the basics of hypothesis testing in Python. I introduced you\n",
    "to the SciPy library, which you can use for the students t test. We've discussed some of the practical\n",
    "issues which arise from looking for statistical significance. There's much more to learn about hypothesis\n",
    "testing, for instance, there are different tests used, depending on the shape of your data and different\n",
    "ways to report results instead of just p-values such as confidence intervals or bayesian analyses. But this\n",
    "should give you a basic idea of where to start when comparing two populations for differences, which is a\n",
    "common task for data scientists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
