{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In `Module Three`** \n",
    "\n",
    "**you will explore ways of measuring the importance or centrality of a node in a network. You will cover several different centrality measures including Degree, Closeness, and Betweenness centrality, Page Rank, and Hubs and Authorities. You will learn about the assumptions each measure makes, the algorithms we can use to compute them, and the different functions available on NetworkX to measure centrality. You will also compare the ranking of nodes by centrality produced by the different measures. In the assignment, you will practice choosing the most appropriate centrality measure on a real-world setting, where you are tasked with choosing a person from a social network who should be given a promotional voucher in order to maximize the impact of the promotion on the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Degree and Closeness Centrality.\n",
    "![1](1.png) \n",
    "![2](2.png)\n",
    "![3](3.png)\n",
    "![4](4.png)\n",
    "![5](5.png)\n",
    "![6](6.png)\n",
    "![7](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](8.png)\n",
    "**How degree Centrality Works?**\n",
    "\n",
    "So, we'll say that the degree centrality of a node V is going to be the ratio between its degree and the number of nodes in the graph minus one. So, in this case a node would have a centrality of one if it's connected to every single node in the network and a centrality of zero if it's connected to no node in the network. And so, this measure goes between zero and one, with one being the case where you're most connected.\n",
    "\n",
    "![9](9.png)\n",
    "![10](10.png)\n",
    "![11](11.png)\n",
    "\n",
    "**How Closeness Centrality works?**\n",
    "- The assumption here is that nodes that are important are going to be a short distance away from all other nodes in the network. Recall that we measure distance between two nodes by looking at the length of the shortest path between them.\n",
    "\n",
    "\n",
    "- And so, the way we're going to define the closeness centrality of node V is going to be by taking the ratio of the number of nodes in the network minus one divided by the sum over all the other nodes in the network, and the distance between node V and those nodes. So, that's the sum and the denominator in the definition of centrality.\n",
    "\n",
    "![12](12.png)\n",
    "![13](13.png)\n",
    "\n",
    "- `First option`:  we can simply only consider the nodes that L can actually reach in order to measure its closeness centrality. So, the way this would work is that we define this set R(L) to be the set of nodes that L can actually reach and we define the closeness centrality of L to be the ratio of the number of nodes that L can reach divided by the sum of the distances from L to all the nodes that L can actually reach.\n",
    "\n",
    "\n",
    "- And so, for node L here, this would be simply one, because L can only reach node M so R(L) here is just the set M is just the node M and L can reach M in just one step. So, the closeness centrality of L here, would be one over one.\n",
    "![14](14.png)\n",
    "\n",
    "- `In option 2`: we again only consider the nodes that L can reach, but then, we normalize by the fraction of nodes that L can reach. So, the way this looks here is that when we compute the closeness centrality of L, we have the same ratio of R(L) over the sum. But now, we're going to multiply that ratio, the fraction of nodes that L can reach, R(L), divided by the number of nodes in the graph minus one. So basically, we're normalizing by the fraction of nodes that L can reach.\n",
    "\n",
    "\n",
    "- And so, if L cannot reach many nodes we're going to be multiplying these other fraction by a very small number. And so, in this case if we do that we find that L has a closeness centrality of 0. 071 which is more reasonable than defined to be one.\n",
    "\n",
    "`Note here`\n",
    "- That in this new definition when we're normalizing, we're not changing the definition of closeness centrality when the graph is connected, where in every node can reach every other node. That's because when that's the case R(L) for node L would equal M minus one and this formula that you see here would be the exact same formula that we had before. So, we're not changing the definition. This definition can still apply even if the graph is completely connected. \n",
    "\n",
    "![15](15.png)\n",
    "![16](16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Betweenness Centrality.\n",
    "![17](17.png)\n",
    "![18](18.png)\n",
    "**- Actually, we're going to find that there are different ways in which we can pick the specific s and t's that we use to compute the centrality node v. But we'll talk about that next. Basic idea here is that a node v has high betweenness centrality if it shows up in many of the shortest paths of nodes s and t.**\n",
    "\n",
    "![19](19.png)\n",
    "![20](20.png)\n",
    "![21](21.png)\n",
    "![22](22.png)\n",
    "![23](23.png)\n",
    "![24](24.png)\n",
    "![25](25.png)\n",
    "### Ways To Reduce Computing Time.\n",
    "![26](26.png)\n",
    "![27](27.png)\n",
    "\n",
    "- The other thing that sometimes is useful is that sometimes you rather compute the betweenness centrality based on `two subgroups` in the network, not necessarily looking at all potential pairs of nodes. But you maybe really care about two groups communicating with each other. So you want to find what are the most important nodes in this network that tend to show up in the shortest paths between a group of source nodes and a group of target nodes? \n",
    "\n",
    "\n",
    "- And so to do this in network x, you can use the function betweenness centrality subset, in which you pass the graph and then you pass the set of source nodes and the set of target nodes.\n",
    "\n",
    "\n",
    "- `What the meaning of these source nodes and target nodes is ?` that when we select the nodes s, t to compute the centrality of all the nodes, we're always going to choose s from the set of source nodes, and t from the set of target nodes, rather than selecting all possible pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about The Importance of Edges ?\n",
    "![28](28.png)\n",
    "![29](29.png)\n",
    "![30](30.png)\n",
    "![31](31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Basic Page Rank.\n",
    "We've been thinking about how to measure the importance or the centrality of a node in a network. And today we're going to talk about another way of doing this. It's called PageRank and it was developed by the Google founders when they were thinking about how to measure the importance of webpages using the hyperlink network structure of the web. And the basic idea, is that PageRank will assign a score of importance to every single node. And the assumption that it makes, is that important nodes are those that have many in-links from important pages or important other nodes.\n",
    "![32](32.png)\n",
    "![33](33.png)\n",
    "![34](34.png)\n",
    "![35](35.png)\n",
    "![36](36.png)\n",
    "![37](37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`True or False`: In directed networks, nodes with higher `in-degree`always have higher PageRank ?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "`False` Nodes with fewer in-degrees may have a high Page Rank when they are connected to a more important node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![38](38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Scaled Page Rank.\n",
    "![39](39.png)\n",
    "![40](40.png)\n",
    "![41](41.png)\n",
    "![42](42.png)\n",
    "![43](43.png)\n",
    "![44](44.png)\n",
    "![45](45.png)\n",
    "![46](46.png)\n",
    "![47](47.png)\n",
    "![48](48.png)\n",
    "![49](49.png)\n",
    "\n",
    "**- So again, at every step, what we used to do before was to always follow the edges. What we're going to do now is that at every step, we're either going to follow the edges with probability alpha. Or we are going to forget about the edges, and choose a random node, and go to it with probability one minus alpha, and we're going to repeat this k times.**\n",
    "\n",
    "![50](50.png)\n",
    "![51](51.png)\n",
    "\n",
    "- And so if we look at the Scaled PageRank value for each one of these nodes, for k very large and alpha parameter of .08, these are the values that we get. So, what you find is that F and G still have a very high PageRank compared to the other notes. But the other nodes don't have a PageRank value of 0. And if we you at the PageRank of all the other nodes, A through E, you find that it follows the same type of ordering that it did before.\n",
    "\n",
    "\n",
    "- So B still has the highest value of Scaled PageRank followed by C, followed by D and A, which roughly get the same value, and then followed by node E. And so F and G still have high PageRank, but not all of the PageRank. And this damping parameter works better for large networks like the web or very large social networks. And this small networks sometimes, it doesn't work very well. In this particular example that I showed you, it works well. So it serves the purpose of showing you how it works, but it's much better for very, very large network.\n",
    "![52](52.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Hubs and Authorities.\n",
    "![53](53.png)\n",
    "- The difference between this new way `hubs and authorities` and `PageRank`?\n",
    "\n",
    "is that rather than taking the full network, we're starting with a subset of the network. Again, looking at first just the root set, the web pages that may be relevant, and then any page that links to it. And so these will be just the subset of the full network of the web.\n",
    "\n",
    "![54](54.png)\n",
    "## EX:\n",
    "![55](55.png)\n",
    "![56](56.png)\n",
    "![57](57.png)\n",
    "![58](58.png)\n",
    "![59](59.png)\n",
    "![60](60.png)\n",
    "![61](61.png)\n",
    "![62](62.png)\n",
    "![63](63.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Centrality Examples.\n",
    "- Now that we've seen a number of different ways of finding central nodes in a network, today, we're going to look at an example where we compare how the different centrality measures that we've looked at, rank nodes differently. And so, we're going to be looking at this particular network and we're going to run the different algorithms that we looked at on this particular network.\n",
    "![64](64.png)\n",
    "![65](65.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
